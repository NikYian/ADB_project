{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-07T15:26:08.652GMT\n",
      "Total Duration: 42.028999999999996 seconds\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Replace these placeholders with your Spark server URL and application ID\n",
    "server_url = \"http://snf-39553.ok-kno.grnetcloud.net:18080\"\n",
    "app_id = \"application_1704639854242_0008\"\n",
    "\n",
    "# Define the endpoint for job details\n",
    "job_details_endpoint = f\"{server_url}/api/v1/applications/{app_id}/jobs\"\n",
    "\n",
    "# Make a GET request to the job details endpoint\n",
    "response = requests.get(job_details_endpoint)\n",
    "\n",
    "# Check if the request was successful (HTTP status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    jobs_data = json.loads(response.content.decode('utf-8'))\n",
    "\n",
    "    sum = 0\n",
    "    for job in jobs_data:\n",
    "# Convert submissionTime and completionTime to datetime objects\n",
    "        start_time = datetime.strptime(job[\"submissionTime\"], \"%Y-%m-%dT%H:%M:%S.%fGMT\")\n",
    "        end_time = datetime.strptime(job[\"completionTime\"], \"%Y-%m-%dT%H:%M:%S.%fGMT\")\n",
    "\n",
    "        # Calculate the duration in seconds\n",
    "        duration_seconds = (end_time - start_time).total_seconds()\n",
    "        sum += duration_seconds\n",
    "\n",
    "    print(f\"Total Duration: {sum} seconds\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve job details. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Spark \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/06 20:46:21 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('Project') \\\n",
    "        .config(\"spark.master\", \"yarn\") \\\n",
    "        .config(\"spark.executor.instances\", \"4\") \\\n",
    "        .config(\"spark.executor.cores\", \"1\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŒñŒ∑œÑŒøœçŒºŒµŒΩŒø 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df1.union(df2)\n",
    "\n",
    "df = df.withColumn(\"Date Rptd\", to_date(col(\"Date Rptd\"), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df = df.withColumn(\"DATE OCC\", to_date(col(\"DATE OCC\"), 'MM/dd/yyyy hh:mm:ss a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2135495"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "852950"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2988445"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of rows in the DataFrame:\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŒñŒ∑œÑŒøœçŒºŒµŒΩŒø 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 199:============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+----------+\n",
      "|Year|Month|crime_total|row_number|\n",
      "+----+-----+-----------+----------+\n",
      "|2010|3    |17595      |1         |\n",
      "|2010|7    |17520      |2         |\n",
      "|2010|5    |17338      |3         |\n",
      "|2011|8    |17139      |1         |\n",
      "|2011|5    |17050      |2         |\n",
      "|2011|3    |16951      |3         |\n",
      "|2012|8    |17696      |1         |\n",
      "|2012|10   |17477      |2         |\n",
      "|2012|5    |17391      |3         |\n",
      "|2013|8    |17329      |1         |\n",
      "|2013|7    |16714      |2         |\n",
      "|2013|5    |16671      |3         |\n",
      "|2014|10   |12789      |1         |\n",
      "|2014|7    |12696      |2         |\n",
      "|2014|9    |12498      |3         |\n",
      "|2015|8    |18951      |1         |\n",
      "|2015|10   |18916      |2         |\n",
      "|2015|7    |18528      |3         |\n",
      "|2016|8    |19779      |1         |\n",
      "|2016|10   |19615      |2         |\n",
      "+----+-----+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, count, row_number\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df1.union(df2)\n",
    "\n",
    "df = df.withColumn(\"Date Rptd\", to_date(col(\"Date Rptd\"), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df = df.withColumn(\"DATE OCC\", to_date(col(\"DATE OCC\"), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "\n",
    "date_rptd = df.select('Date Rptd')\n",
    "date_rptd = date_rptd.withColumn(\"Year\", year(\"Date Rptd\")).withColumn(\"Month\", month(\"Date Rptd\")).drop(\"Date Rptd\")\n",
    "\n",
    "crime_total = date_rptd.groupBy(\"Year\", \"Month\").agg(count(\"*\").alias(\"crime_total\"))\n",
    "\n",
    "# Define a window specification to partition by the \"Year\" column and order by the \"crime_total\" column\n",
    "window_spec = Window().partitionBy(\"Year\").orderBy(col(\"crime_total\").desc())\n",
    "\n",
    "# Use the row_number function to assign row numbers within each group\n",
    "df_sorted = crime_total.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to keep only the top three within each group\n",
    "df_top_three_DF = df_sorted.filter(col(\"row_number\") <= 3)\n",
    "\n",
    "df_top_three_DF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['hadoop', 'fs', '-copyToLocal', 'hdfs://okeanos-master:54310/user/user/results/q1Dt.csv', '/home/user/Project/results/'], returncode=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_top_three_DF \\\n",
    "  .coalesce(1) \\\n",
    "  .write \\\n",
    "  .mode('overwrite') \\\n",
    "  .option('header', 'true') \\\n",
    "  .csv('results/q1Dt.csv')\n",
    "# df_top_three_DF.write.csv(\"results/q1Dt.csv\", header=True,  mode=\"overwrite\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "hdfs_path = \"hdfs://okeanos-master:54310/user/user/results/q1Dt.csv\"\n",
    "local_path = \"/home/user/Project/results/\"\n",
    "\n",
    "subprocess.run([\"hadoop\", \"fs\", \"-copyToLocal\", hdfs_path, local_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:================================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+----------+\n",
      "|Year|Month|crime_total|row_number|\n",
      "+----+-----+-----------+----------+\n",
      "|2010|3    |17595      |1         |\n",
      "|2010|7    |17520      |2         |\n",
      "|2010|5    |17338      |3         |\n",
      "|2011|8    |17139      |1         |\n",
      "|2011|5    |17050      |2         |\n",
      "|2011|3    |16951      |3         |\n",
      "|2012|8    |17696      |1         |\n",
      "|2012|10   |17477      |2         |\n",
      "|2012|5    |17391      |3         |\n",
      "|2013|8    |17329      |1         |\n",
      "|2013|7    |16714      |2         |\n",
      "|2013|5    |16671      |3         |\n",
      "|2014|10   |12789      |1         |\n",
      "|2014|7    |12696      |2         |\n",
      "|2014|9    |12498      |3         |\n",
      "|2015|8    |18951      |1         |\n",
      "|2015|10   |18916      |2         |\n",
      "|2015|7    |18528      |3         |\n",
      "|2016|8    |19779      |1         |\n",
      "|2016|10   |19615      |2         |\n",
      "+----+-----+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary SQL table\n",
    "df.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# Write the SQL query\n",
    "sql_query = \"\"\"\n",
    "    SELECT Year, Month, crime_total, row_number\n",
    "    FROM (\n",
    "        SELECT Year, Month, crime_total,\n",
    "               ROW_NUMBER() OVER (PARTITION BY Year ORDER BY crime_total DESC) AS row_number\n",
    "        FROM (\n",
    "            SELECT YEAR(`Date Rptd`) AS Year, MONTH(`Date Rptd`) AS Month, COUNT(*) AS crime_total\n",
    "            FROM crime_data\n",
    "            GROUP BY Year, Month\n",
    "        ) tmp\n",
    "    ) tmp2\n",
    "    WHERE row_number <= 3\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "df_top_three_sql = spark.sql(sql_query)\n",
    "\n",
    "# Show the result\n",
    "df_top_three_sql.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['hadoop', 'fs', '-copyToLocal', 'hdfs://okeanos-master:54310/user/user/results/q1SQL.csv', '/home/user/Project/results/'], returncode=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_top_three_sql.write.csv(\"results/q1SQL.csv\", header=True,  mode=\"overwrite\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "hdfs_path = \"hdfs://okeanos-master:54310/user/user/results/q1SQL.csv\"\n",
    "local_path = \"/home/user/Project/results/\"\n",
    "\n",
    "subprocess.run([\"hadoop\", \"fs\", \"-copyToLocal\", hdfs_path, local_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 129:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrames are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "is_same = df_top_three_DF.exceptAll(df_top_three_sql).count() == 0\n",
    "if is_same:\n",
    "    print(\"The DataFrames are identical.\")\n",
    "else:\n",
    "    print(\"The DataFrames are different.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŒñŒ∑œÑŒøœçŒºŒµŒΩŒø 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|TIME OCC|Premis Cd|\n",
      "+--------+---------+\n",
      "|13:50:00|      501|\n",
      "|00:45:00|      101|\n",
      "|15:15:00|      103|\n",
      "|01:50:00|      101|\n",
      "|21:00:00|      103|\n",
      "|16:50:00|      404|\n",
      "|20:05:00|      101|\n",
      "|21:00:00|      710|\n",
      "|02:30:00|      108|\n",
      "|21:00:00|      710|\n",
      "|14:45:00|      101|\n",
      "|20:00:00|      101|\n",
      "|02:45:00|      102|\n",
      "|17:45:00|      738|\n",
      "|20:30:00|      102|\n",
      "|17:35:00|      103|\n",
      "|12:25:00|      502|\n",
      "|11:00:00|      101|\n",
      "|20:00:00|      502|\n",
      "|18:20:00|      102|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, from_unixtime, date_format\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=False).select(\"TIME OCC\",\"Premis Cd\")\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=False).select(\"TIME OCC\",\"Premis Cd\")\n",
    "\n",
    "df = df1.union(df2)\n",
    "\n",
    "\n",
    "# Convert the 'TIME OCC' column to a timestamp\n",
    "df = df.withColumn(\n",
    "    \"TIME OCC\",\n",
    "    from_unixtime(unix_timestamp(col(\"TIME OCC\"), \"HHmm\")).cast(\"timestamp\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"TIME OCC\",\n",
    "    date_format(col(\"TIME OCC\").cast(\"timestamp\"), \"HH:mm:ss\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Premis Cd\",\n",
    "    col(\"Premis Cd\").cast(\"int\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================================>             (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+------+\n",
      "|Morning|Afternoon|Evening|Night |\n",
      "+-------+---------+-------+------+\n",
      "|123748 |148077   |186896 |237137|\n",
      "+-------+---------+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, sum\n",
    "\n",
    "filtered_df = df.filter(col(\"Premis Cd\") == 101).select(\"TIME OCC\")\n",
    "\n",
    "# Define time intervals\n",
    "morning_interval = ((col(\"TIME OCC\") >= \"05:00:00\") & (col(\"TIME OCC\") < \"12:00:00\"))\n",
    "afternoon_interval = ((col(\"TIME OCC\") >= \"12:00:00\") & (col(\"TIME OCC\") < \"17:00:00\"))\n",
    "evening_interval = ((col(\"TIME OCC\") >= \"17:00:00\") & (col(\"TIME OCC\") < \"21:00:00\"))\n",
    "night_interval = ((col(\"TIME OCC\") >= \"21:00:00\") | (col(\"TIME OCC\") < \"05:00:00\"))\n",
    "\n",
    "# Apply conditions and sum within each interval\n",
    "result_df = filtered_df.groupBy().agg(\n",
    "    sum(when(morning_interval, 1).otherwise(0)).alias(\"Morning\"),\n",
    "    sum(when(afternoon_interval, 1).otherwise(0)).alias(\"Afternoon\"),\n",
    "    sum(when(evening_interval, 1).otherwise(0)).alias(\"Evening\"),\n",
    "    sum(when(night_interval, 1).otherwise(0)).alias(\"Night\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[('Afternoon', 126476), ('Night', 205687), ('Morning', 107927), ('Evening', 165672)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/06 19:16:27 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Afternoon', 148077), ('Night', 237137), ('Morning', 123748), ('Evening', 186896)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import csv\n",
    "\n",
    "# Load the first CSV file into an RDD\n",
    "rdd1 = spark.textFile(\"hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv\") \\\n",
    "    .map(lambda x: next(csv.reader([x])))\n",
    "    \n",
    "header1 = rdd1.first()\n",
    "rdd1 = rdd1.filter(lambda row: row != header1)\n",
    "\n",
    "# Load the second CSV file into an RDD\n",
    "rdd2 = spark.textFile(\"hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv\") \\\n",
    "    .map(lambda x: next(csv.reader([x])))\n",
    "\n",
    "    \n",
    "header2 = rdd2.first()  \n",
    "rdd2 = rdd2.filter(lambda row: row != header2)\n",
    "\n",
    "# Merge the two RDDs\n",
    "rdd = rdd1.union(rdd2)\n",
    "rdd = rdd.map(lambda col: (col[3], col[14]))\n",
    "\n",
    "\n",
    "filtered_rdd = rdd.filter(lambda row: (row[1] == '101') or (row[1] == 101) )\n",
    "\n",
    "def get_interval(time_occ):\n",
    "    # Convert the time_occ to a datetime object for easier comparison\n",
    "    time_object = datetime.datetime.strptime(time_occ, \"%H%M\")\n",
    "\n",
    "    if datetime.time(5, 0) <= time_object.time() < datetime.time(12, 0):\n",
    "        return \"Morning\"\n",
    "    elif datetime.time(12, 0) <= time_object.time() < datetime.time(17, 0):\n",
    "        return \"Afternoon\"\n",
    "    elif datetime.time(17, 0) <= time_object.time() < datetime.time(21, 0):\n",
    "        return \"Evening\"\n",
    "    elif (datetime.time(21, 0) <= time_object.time()) or  (time_object.time() < datetime.time(5, 0)):\n",
    "        return \"Night\"\n",
    "\n",
    "    \n",
    "# Map each row to a tuple of (interval, 1)\n",
    "mapped_rdd = filtered_rdd.map(lambda col: (get_interval(col[0]), 1))\n",
    "\n",
    "# Reduce by key to sum occurrences within each interval\n",
    "result_rdd = mapped_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "print(result_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŒñŒ∑œÑŒøœçŒºŒµŒΩŒø 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, regexp_replace\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df1.union(df2)\n",
    "df = df.select('Date Rptd', 'Vict Descent','LAT', 'LON').filter(col('Vict Descent').isNotNull())\n",
    "df = df.withColumn(\"Date Rptd\", to_date(col(\"Date Rptd\"), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df = df.withColumn(\"Year\", year(\"Date Rptd\")).drop(\"Date Rptd\").filter(col(\"Year\") == 2015).drop('Year')\n",
    "\n",
    "income = spark.read.csv('hdfs://okeanos-master:54310/user/project/income/LA_income_2015.csv', header=True, inferSchema=True)\n",
    "income = income.withColumn(\"Estimated Median Income\", regexp_replace(\"Estimated Median Income\", \"[^0-9]\", \"\").cast(\"int\"))\n",
    "geocoding = spark.read.csv('hdfs://okeanos-master:54310/user/project/revgecoding.csv', header=True, inferSchema=True)\n",
    "\n",
    "geocoding = geocoding \\\n",
    "    .withColumnRenamed(\"LAT\", \"LAT_g\") \\\n",
    "    .withColumnRenamed(\"LON\", \"LON_g\") \n",
    "\n",
    "df = df.join(geocoding, (df.LAT == geocoding.LAT_g) & (df.LON == geocoding.LON_g)).drop(\"LAT\",\"LON\",\"LAT_g\",\"LON_g\")\n",
    "\n",
    "# non_matching_rows = df.join(geocoding, (df.LAT == geocoding.LAT_g) & (df.LON == geocoding.LON_g), \"left_anti\")\n",
    "\n",
    "distinct_geocoding = geocoding.select(\"ZIPcode\").distinct()\n",
    "filtered_income = income.join(distinct_geocoding, income[\"Zip Code\"] == distinct_geocoding[\"ZIPcode\"])\n",
    "filtered_income = filtered_income.orderBy(col(\"Estimated Median Income\").desc())\n",
    "\n",
    "# Get top 3 zip codes with highest Estimated Median Income\n",
    "top3 = filtered_income.limit(3)\n",
    "\n",
    "# Get bottom 3 zip codes with lowest Estimated Median Income\n",
    "filtered_income = filtered_income.orderBy(col(\"Estimated Median Income\"))\n",
    "tail3 = filtered_income.limit(3)\n",
    "\n",
    "join_top3 = df.join(top3, (df.ZIPcode == top3.ZIPcode))\n",
    "count_top3 = join_top3.groupBy(\"Vict Descent\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "join_tail3 = df.join(tail3, (df.ZIPcode == top3.ZIPcode))\n",
    "count_tail3 = join_tail3.groupBy(\"Vict Descent\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "descent_mapping = {\n",
    "    \"A\": \"Other Asian\",\n",
    "    \"B\": \"Black\",\n",
    "    \"C\": \"Chinese\",\n",
    "    \"D\": \"Cambodian\",\n",
    "    \"F\": \"Filipino\",\n",
    "    \"G\": \"Guamanian\",\n",
    "    \"H\": \"Hispanic/Latin/Mexican\",\n",
    "    \"I\": \"American Indian/Alaskan Native\",\n",
    "    \"J\": \"Japanese\",\n",
    "    \"K\": \"Korean\",\n",
    "    \"L\": \"Laotian\",\n",
    "    \"O\": \"Other\",\n",
    "    \"P\": \"Pacific Islander\",\n",
    "    \"S\": \"Samoan\",\n",
    "    \"U\": \"Hawaiian\",\n",
    "    \"V\": \"Vietnamese\",\n",
    "    \"W\": \"White\",\n",
    "    \"X\": \"Unknown\",\n",
    "    \"Z\": \"Asian Indian\"\n",
    "}\n",
    "\n",
    "count_top3 = count_top3.withColumn(\"Victim Descent\", col(\"Vict Descent\").cast(\"string\")).replace(descent_mapping, subset=[\"Victim Descent\"]).drop(\"Vict Descent\")\n",
    "columns_order = [\"Victim Descent\", \"count\"] \n",
    "count_top3 = count_top3.select(columns_order)\n",
    "count_tail3 = count_tail3.withColumn(\"Victim Descent\", col(\"Vict Descent\").cast(\"string\")).replace(descent_mapping, subset=[\"Victim Descent\"]).drop(\"Vict Descent\")\n",
    "count_tail3 = count_tail3.select(columns_order)\n",
    "\n",
    "count_top3.show(truncate=False)\n",
    "\n",
    "count_tail3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------------------------------------------------+-----------------------+-------+\n",
      "|Zip Code|Community                                                  |Estimated Median Income|ZIPcode|\n",
      "+--------+-----------------------------------------------------------+-----------------------+-------+\n",
      "|90021   |Los Angeles (Downtown Fashion District, Downtown Southeast)|12813                  |90021  |\n",
      "|90058   |Los Angeles (Southeast Los Angeles), Vernon                |17018                  |90058  |\n",
      "|90013   |Los Angeles (Downtown Central, Downtown Fashion District)  |19887                  |90013  |\n",
      "+--------+-----------------------------------------------------------+-----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tail3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|Victim Descent        |count|\n",
      "+----------------------+-----+\n",
      "|White                 |312  |\n",
      "|Other                 |102  |\n",
      "|Hispanic/Latin/Mexican|53   |\n",
      "|Unknown               |26   |\n",
      "|Other Asian           |16   |\n",
      "|Black                 |14   |\n",
      "+----------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_top3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|Victim Descent                |count|\n",
      "+------------------------------+-----+\n",
      "|Hispanic/Latin/Mexican        |1503 |\n",
      "|Black                         |1078 |\n",
      "|White                         |690  |\n",
      "|Other                         |382  |\n",
      "|Other Asian                   |100  |\n",
      "|Unknown                       |63   |\n",
      "|Korean                        |7    |\n",
      "|American Indian/Alaskan Native|3    |\n",
      "|Japanese                      |3    |\n",
      "|Chinese                       |2    |\n",
      "|Filipino                      |1    |\n",
      "+------------------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_tail3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 380:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|Vict Descent|count|\n",
      "+------------+-----+\n",
      "|           H| 1503|\n",
      "|           B| 1078|\n",
      "|           W|  690|\n",
      "|           O|  382|\n",
      "|           A|  100|\n",
      "|           X|   63|\n",
      "|           K|    7|\n",
      "|           I|    3|\n",
      "|           J|    3|\n",
      "|           C|    2|\n",
      "|           F|    1|\n",
      "+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_tail3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŒñŒ∑œÑŒøœçŒºŒµŒΩŒø 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/06 20:28:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/06 20:28:18 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "spark = SparkSession.builder.config(\n",
    "    \"spark.archives\",  # 'spark.yarn.dist.archives' in YARN.\n",
    "    \"pyspark_conda_env.tar.gz#environment\")\\\n",
    "        .config(\"spark.master\", \"yarn\") \\\n",
    "        .config(\"spark.executor.instances\", \"4\") \\\n",
    "        .config(\"spark.executor.cores\", \"1\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-----+\n",
      "|Year|Mean_Distance (km)|    #|\n",
      "+----+------------------+-----+\n",
      "|2010|             2.784| 8161|\n",
      "|2011|             2.790| 7225|\n",
      "|2012|             2.834| 6521|\n",
      "|2013|             2.830| 5851|\n",
      "|2014|             2.713| 4257|\n",
      "|2015|             2.706| 6729|\n",
      "|2016|             2.718| 8094|\n",
      "|2017|             2.721| 7780|\n",
      "|2018|             2.736| 7414|\n",
      "|2019|             2.741| 7135|\n",
      "|2020|             2.688| 8492|\n",
      "|2021|             2.696|12659|\n",
      "|2022|             2.612|10067|\n",
      "|2023|             2.556| 8796|\n",
      "+----+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+-----+\n",
      "|        DIVISION|Mean_Distance (km)|    #|\n",
      "+----------------+------------------+-----+\n",
      "|     77TH STREET|             2.698|16547|\n",
      "|       SOUTHEAST|             2.104|12901|\n",
      "|          NEWTON|             2.015| 9608|\n",
      "|       SOUTHWEST|             2.700| 8633|\n",
      "|      HOLLENBECK|             2.650| 6111|\n",
      "|          HARBOR|             4.086| 5432|\n",
      "|         RAMPART|             1.577| 4989|\n",
      "|         MISSION|             4.716| 4459|\n",
      "|         OLYMPIC|             1.835| 4326|\n",
      "|       NORTHEAST|             3.906| 3846|\n",
      "|        FOOTHILL|             3.821| 3756|\n",
      "| NORTH HOLLYWOOD|             2.715| 3642|\n",
      "|       HOLLYWOOD|             1.454| 3551|\n",
      "|         CENTRAL|             1.137| 3466|\n",
      "|        WILSHIRE|             2.321| 3420|\n",
      "|     WEST VALLEY|             3.527| 2786|\n",
      "|         PACIFIC|             3.735| 2743|\n",
      "|        VAN NUYS|             2.215| 2645|\n",
      "|      DEVONSHIRE|             4.019| 2501|\n",
      "|         TOPANGA|             3.482| 2310|\n",
      "|WEST LOS ANGELES|             4.262| 1509|\n",
      "+----------------+------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, year,udf, mean,format_number, count\n",
    "from pyspark.sql.types import  FloatType\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df1.union(df2)\n",
    "df = df.withColumn(\"Date Rptd\", to_date(col(\"Date Rptd\"), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df = df.withColumn(\"Year\", year(\"Date Rptd\")).drop(\"Date Rptd\")\n",
    "\n",
    "df = df.select(\"DR_NO\",\"Year\",\"AREA \",\"Weapon Used Cd\", \"LAT\", \"LON\")\n",
    "\n",
    "# filter fire arm crimes\n",
    "df = df.filter(df[\"Weapon Used Cd\"].cast(\"int\").between(100, 199))\n",
    "\n",
    "# remove Null Island entries\n",
    "df = df.filter((df[\"LAT\"] != 0) & (df[\"LON\"] != 0))\n",
    "\n",
    "police_stations = spark.read.csv('hdfs://okeanos-master:54310/user/project/LAPD_Police_Stations.csv', header=True, inferSchema=True)\n",
    "\n",
    "police_stations = police_stations.select(\"DIVISION\",\"X\", \"Y\",\"PREC\")\n",
    "\n",
    "joined_df = df.join(police_stations, (df[\"AREA \"] == police_stations.PREC)).drop(\"AREA \", \"Weapon Used Cd\",\"PREC\")\n",
    "\n",
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    distance_km = geodesic((lat1, lon1), (lat2, lon2)).km\n",
    "    rounded_distance_km = round(distance_km, 3)\n",
    "    return rounded_distance_km\n",
    "\n",
    "get_distance = udf(get_distance, FloatType())\n",
    "\n",
    "distance_df = joined_df.withColumn(\"Distance\", get_distance(col(\"LAT\"), col(\"LON\"), col(\"Y\"), col(\"X\")))\n",
    "\n",
    "result_a = distance_df.groupBy(\"Year\").agg(\n",
    "    mean(\"Distance\").alias(\"Mean_Distance (km)\"),\n",
    "    count(\"*\").alias(\"#\")\n",
    ").orderBy(col(\"Year\"))\n",
    "\n",
    "result_a = result_a.withColumn(\"Mean_Distance (km)\", format_number(\"Mean_Distance (km)\", 3))\n",
    "\n",
    "result_b = distance_df.groupBy(\"DIVISION\").agg(\n",
    "    mean(\"Distance\").alias(\"Mean_Distance (km)\"),\n",
    "    count(\"*\").alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "result_b = result_b.withColumn(\"Mean_Distance (km)\", format_number(\"Mean_Distance (km)\", 3))\n",
    "\n",
    "result_a.show()\n",
    "\n",
    "result_b.show(result_b.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+----------------+----+\n",
      "|        DIVISION|             X|               Y|PREC|\n",
      "+----------------+--------------+----------------+----+\n",
      "|          HARBOR|-118.289241553|33.7576608970001|   5|\n",
      "|       SOUTHEAST|-118.275394206|33.9386273800001|  18|\n",
      "|     77TH STREET|-118.277669655|33.9703073800001|  12|\n",
      "|         PACIFIC|-118.419841576|33.9916553210001|  14|\n",
      "|       SOUTHWEST|-118.305141563|34.0105753400001|   3|\n",
      "|          NEWTON|-118.256118891|    34.012355905|  13|\n",
      "|         CENTRAL|-118.247294123|      34.0440195|   1|\n",
      "|WEST LOS ANGELES|-118.450779541|34.0437774120001|   8|\n",
      "|      HOLLENBECK|-118.213067956|    34.045008769|   4|\n",
      "|        WILSHIRE|-118.342829525|    34.046747682|   7|\n",
      "|         OLYMPIC|-118.291175911|    34.050208529|  20|\n",
      "|         RAMPART|-118.266979649|    34.056690437|   2|\n",
      "|       HOLLYWOOD| -118.33066931|    34.095833225|   6|\n",
      "|       NORTHEAST|-118.249414484|    34.119200666|  11|\n",
      "| NORTH HOLLYWOOD|-118.385859348|34.1716939300001|  15|\n",
      "|        VAN NUYS|-118.445225709|34.1837432730001|   9|\n",
      "|     WEST VALLEY|-118.547454438|    34.193397227|  10|\n",
      "|         TOPANGA|-118.599636542|    34.221376654|  21|\n",
      "|        FOOTHILL|-118.410417183|34.2530912220001|  16|\n",
      "|      DEVONSHIRE|-118.531373363|    34.256969059|  17|\n",
      "+----------------+--------------+----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "police_stations.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, year,udf, mean,format_number, count,row_number\n",
    "from pyspark.sql.types import  FloatType\n",
    "from geopy.distance import geodesic\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# # Read the CSVs file into a DataFrames\n",
    "# df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=True)\n",
    "# df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=True)\n",
    "\n",
    "# df = df1.union(df2)\n",
    "# df = df.withColumn(\"Date Rptd\", to_date(col(\"Date Rptd\"), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "# df = df.withColumn(\"Year\", year(\"Date Rptd\")).drop(\"Date Rptd\")\n",
    "\n",
    "# df = df.select(\"DR_NO\",\"Year\",\"AREA \",\"Weapon Used Cd\", \"LAT\", \"LON\")\n",
    "# # filter fire arm crimes\n",
    "# df = df.filter(df[\"Weapon Used Cd\"].cast(\"int\").between(100, 199))\n",
    "\n",
    "# # remove Null Island entries\n",
    "# df = df.filter((df[\"LAT\"] != 0) & (df[\"LON\"] != 0))\n",
    "\n",
    "# police_stations = spark.read.csv('hdfs://okeanos-master:54310/user/project/LAPD_Police_Stations.csv', header=True, inferSchema=True)\n",
    "\n",
    "# police_stations = police_stations.select(\"DIVISION\",\"X\", \"Y\",\"PREC\")\n",
    "\n",
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    distance_km = geodesic((lat1, lon1), (lat2, lon2)).km\n",
    "    rounded_distance_km = round(distance_km, 3)\n",
    "    return rounded_distance_km\n",
    "\n",
    "get_distance = udf(get_distance, FloatType())\n",
    "\n",
    "joined_df = df.crossJoin(police_stations) \\\n",
    "    .withColumn(\"Distance\", get_distance(col(\"LAT\"), col(\"LON\"), col(\"Y\"), col(\"X\"))) \n",
    "    \n",
    "window_spec = Window.partitionBy(\"DR_NO\").orderBy(\"Distance\")\n",
    "result_df = joined_df.withColumn(\"row_number\", row_number().over(window_spec)).filter(col(\"row_number\") == 1)\n",
    "\n",
    "# Drop the additional column used for window function\n",
    "result_df = result_df.drop(\"row_number\")\n",
    "\n",
    "result_a = result_df.groupBy(\"Year\").agg(\n",
    "    mean(\"Distance\").alias(\"Mean Distance From Closest (km)\"),\n",
    "    count(\"*\").alias(\"#\")\n",
    ").orderBy(col(\"Year\"))\n",
    "\n",
    "result_a = result_a.withColumn(\"Mean Distance From Closest (km)\", format_number(\"Mean Distance From Closest (km)\", 3))\n",
    "\n",
    "result_b = result_df.groupBy(\"DIVISION\").agg(\n",
    "    mean(\"Distance\").alias(\"Mean Distance From Closest (km)\"),\n",
    "    count(\"*\").alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "result_b = result_b.withColumn(\"Mean Distance From Closest (km)\", format_number(\"Mean Distance From Closest (km)\", 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------+-----+\n",
      "|Year|Mean Distance From Closest (km)|    #|\n",
      "+----+-------------------------------+-----+\n",
      "|2010|                          2.435| 8161|\n",
      "|2011|                          2.458| 7225|\n",
      "|2012|                          2.505| 6521|\n",
      "|2013|                          2.459| 5851|\n",
      "|2014|                          2.325| 4257|\n",
      "|2015|                          2.388| 6729|\n",
      "|2016|                          2.426| 8094|\n",
      "|2017|                          2.390| 7780|\n",
      "|2018|                          2.411| 7414|\n",
      "|2019|                          2.430| 7135|\n",
      "|2020|                          2.382| 8492|\n",
      "|2021|                          2.352| 9746|\n",
      "|2022|                          2.314|10031|\n",
      "|2023|                          2.272| 8794|\n",
      "+----+-------------------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------------------+-----+\n",
      "|        DIVISION|Mean Distance From Closest (km)|    #|\n",
      "+----------------+-------------------------------+-----+\n",
      "|     77TH STREET|                          1.721|13287|\n",
      "|       SOUTHWEST|                          2.279|11189|\n",
      "|       SOUTHEAST|                          2.210|10857|\n",
      "|          NEWTON|                          1.569| 7150|\n",
      "|        WILSHIRE|                          2.446| 6240|\n",
      "|      HOLLENBECK|                          2.638| 6166|\n",
      "|       HOLLYWOOD|                          2.002| 5323|\n",
      "|          HARBOR|                          3.900| 5305|\n",
      "|         OLYMPIC|                          1.664| 5088|\n",
      "|         RAMPART|                          1.397| 4688|\n",
      "|        VAN NUYS|                          2.953| 4589|\n",
      "|        FOOTHILL|                          3.612| 4210|\n",
      "|         CENTRAL|                          1.019| 3571|\n",
      "| NORTH HOLLYWOOD|                          2.721| 3272|\n",
      "|       NORTHEAST|                          3.755| 3092|\n",
      "|     WEST VALLEY|                          2.779| 2700|\n",
      "|         MISSION|                          3.803| 2628|\n",
      "|         PACIFIC|                          3.702| 2521|\n",
      "|         TOPANGA|                          3.041| 2162|\n",
      "|      DEVONSHIRE|                          2.982| 1178|\n",
      "|WEST LOS ANGELES|                          2.768| 1014|\n",
      "+----------------+-------------------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_b.show(result_b.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŒñŒ∑œÑŒøœçŒºŒµŒΩŒø 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/05 14:29:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/05 14:29:44 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('Project') \\\n",
    "        .config(\"spark.master\", \"yarn\") \\\n",
    "        .config(\"spark.executor.instances\", \"4\") \\\n",
    "        .config(\"spark.executor.cores\", \"1\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [ZIPcode#243], [ZIPcode#281], Inner, BuildRight, false\n",
      "   :- Project [Vict Descent#30, ZIPcode#243]\n",
      "   :  +- ShuffledHashJoin [knownfloatingpointnormalized(normalizenanandzero(LAT#43)), knownfloatingpointnormalized(normalizenanandzero(LON#44))], [knownfloatingpointnormalized(normalizenanandzero(LAT_g#247)), knownfloatingpointnormalized(normalizenanandzero(LON_g#252))], Inner, BuildRight\n",
      "   :     :- Exchange hashpartitioning(knownfloatingpointnormalized(normalizenanandzero(LAT#43)), knownfloatingpointnormalized(normalizenanandzero(LON#44)), 200), ENSURE_REQUIREMENTS, [plan_id=153]\n",
      "   :     :  +- Union\n",
      "   :     :     :- Project [Vict Descent#30, LAT#43, LON#44]\n",
      "   :     :     :  +- Filter (((isnotnull(Vict Descent#30) AND (year(cast(gettimestamp(Date Rptd#18, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) as date)) = 2015)) AND isnotnull(LAT#43)) AND isnotnull(LON#44))\n",
      "   :     :     :     +- FileScan csv [Date Rptd#18,Vict Descent#30,LAT#43,LON#44] Batched: false, DataFilters: [isnotnull(Vict Descent#30), (year(cast(gettimestamp(Date Rptd#18, MM/dd/yyyy hh:mm:ss a, Timesta..., Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019...., PartitionFilters: [], PushedFilters: [IsNotNull(Vict Descent), IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<Date Rptd:string,Vict Descent:string,LAT:double,LON:double>\n",
      "   :     :     +- Project [Vict Descent#103, LAT#116, LON#117]\n",
      "   :     :        +- Filter (((isnotnull(Vict Descent#103) AND (year(cast(gettimestamp(Date Rptd#91, MM/dd/yyyy hh:mm:ss a, TimestampType, Some(Europe/Athens), false) as date)) = 2015)) AND isnotnull(LAT#116)) AND isnotnull(LON#117))\n",
      "   :     :           +- FileScan csv [Date Rptd#91,Vict Descent#103,LAT#116,LON#117] Batched: false, DataFilters: [isnotnull(Vict Descent#103), (year(cast(gettimestamp(Date Rptd#91, MM/dd/yyyy hh:mm:ss a, Timest..., Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Prese..., PartitionFilters: [], PushedFilters: [IsNotNull(Vict Descent), IsNotNull(LAT), IsNotNull(LON)], ReadSchema: struct<Date Rptd:string,Vict Descent:string,LAT:double,LON:double>\n",
      "   :     +- Exchange hashpartitioning(knownfloatingpointnormalized(normalizenanandzero(LAT_g#247)), knownfloatingpointnormalized(normalizenanandzero(LON_g#252)), 200), ENSURE_REQUIREMENTS, [plan_id=154]\n",
      "   :        +- Project [LAT#241 AS LAT_g#247, LON#242 AS LON_g#252, ZIPcode#243]\n",
      "   :           +- Filter ((isnotnull(LAT#241) AND isnotnull(LON#242)) AND isnotnull(ZIPcode#243))\n",
      "   :              +- FileScan csv [LAT#241,LON#242,ZIPcode#243] Batched: false, DataFilters: [isnotnull(LAT#241), isnotnull(LON#242), isnotnull(ZIPcode#243)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/user/project/revgecoding.csv], PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), IsNotNull(ZIPcode)], ReadSchema: struct<LAT:double,LON:double,ZIPcode:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[3, string, true]),false), [plan_id=164]\n",
      "      +- TakeOrderedAndProject(limit=3, orderBy=[Estimated Median Income#219 DESC NULLS LAST], output=[Zip Code#213,Community#214,Estimated Median Income#219,ZIPcode#281])\n",
      "         +- BroadcastHashJoin [Zip Code#213], [cast(ZIPcode#281 as int)], Inner, BuildLeft, false\n",
      "            :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=160]\n",
      "            :  +- Project [Zip Code#213, Community#214, cast(regexp_replace(Estimated Median Income#215, [^0-9], , 1) as int) AS Estimated Median Income#219]\n",
      "            :     +- Filter isnotnull(Zip Code#213)\n",
      "            :        +- FileScan csv [Zip Code#213,Community#214,Estimated Median Income#215] Batched: false, DataFilters: [isnotnull(Zip Code#213)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/user/project/income/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Community:string,Estimated Median Income:string>\n",
      "            +- HashAggregate(keys=[ZIPcode#281], functions=[])\n",
      "               +- Exchange hashpartitioning(ZIPcode#281, 200), ENSURE_REQUIREMENTS, [plan_id=157]\n",
      "                  +- HashAggregate(keys=[ZIPcode#281], functions=[])\n",
      "                     +- Filter isnotnull(ZIPcode#281)\n",
      "                        +- FileScan csv [ZIPcode#281] Batched: false, DataFilters: [isnotnull(ZIPcode#281)], Format: CSV, Location: InMemoryFileIndex(1 paths)[hdfs://okeanos-master:54310/user/project/revgecoding.csv], PartitionFilters: [], PushedFilters: [IsNotNull(ZIPcode)], ReadSchema: struct<ZIPcode:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|      Victim Descent|count|\n",
      "+--------------------+-----+\n",
      "|Hispanic/Latin/Me...| 1503|\n",
      "|               Black| 1078|\n",
      "|               White|  690|\n",
      "|               Other|  382|\n",
      "|         Other Asian|  100|\n",
      "|             Unknown|   63|\n",
      "|              Korean|    7|\n",
      "|            Japanese|    3|\n",
      "|American Indian/A...|    3|\n",
      "|             Chinese|    2|\n",
      "|            Filipino|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, regexp_replace\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df1.union(df2)\n",
    "df = df.select('Date Rptd', 'Vict Descent','LAT', 'LON').filter(col('Vict Descent').isNotNull())\n",
    "df = df.withColumn(\"Date Rptd\", to_date(col(\"Date Rptd\"), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df = df.withColumn(\"Year\", year(\"Date Rptd\")).drop(\"Date Rptd\").filter(col(\"Year\") == 2015).drop('Year')\n",
    "\n",
    "income = spark.read.csv('hdfs://okeanos-master:54310/user/project/income/LA_income_2015.csv', header=True, inferSchema=True)\n",
    "income = income.withColumn(\"Estimated Median Income\", regexp_replace(\"Estimated Median Income\", \"[^0-9]\", \"\").cast(\"int\"))\n",
    "geocoding = spark.read.csv('hdfs://okeanos-master:54310/user/project/revgecoding.csv', header=True, inferSchema=True)\n",
    "\n",
    "geocoding = geocoding \\\n",
    "    .withColumnRenamed(\"LAT\", \"LAT_g\") \\\n",
    "    .withColumnRenamed(\"LON\", \"LON_g\") \n",
    "\n",
    "df = df.join(geocoding.hint(\"SHUFFLE_HASH\"), (df.LAT == geocoding.LAT_g) & (df.LON == geocoding.LON_g)).drop(\"LAT\",\"LON\",\"LAT_g\",\"LON_g\")\n",
    "\n",
    "# non_matching_rows = df.join(geocoding, (df.LAT == geocoding.LAT_g) & (df.LON == geocoding.LON_g), \"left_anti\")\n",
    "\n",
    "distinct_geocoding = geocoding.select(\"ZIPcode\").distinct()\n",
    "filtered_income = income.join(distinct_geocoding, income[\"Zip Code\"] == distinct_geocoding[\"ZIPcode\"])\n",
    "filtered_income = filtered_income.orderBy(col(\"Estimated Median Income\").desc())\n",
    "\n",
    "# Get top 3 zip codes with highest Estimated Median Income\n",
    "top3 = filtered_income.limit(3)\n",
    "\n",
    "# Get bottom 3 zip codes with lowest Estimated Median Income\n",
    "filtered_income = filtered_income.orderBy(col(\"Estimated Median Income\"))\n",
    "tail3 = filtered_income.limit(3)\n",
    "\n",
    "join_top3 = df.join(top3, (df.ZIPcode == top3.ZIPcode))\n",
    "join_top3.explain()\n",
    "count_top3 = join_top3.groupBy(\"Vict Descent\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "join_tail3 = df.join(tail3, (df.ZIPcode == top3.ZIPcode))\n",
    "count_tail3 = join_tail3.groupBy(\"Vict Descent\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "descent_mapping = {\n",
    "    \"A\": \"Other Asian\",\n",
    "    \"B\": \"Black\",\n",
    "    \"C\": \"Chinese\",\n",
    "    \"D\": \"Cambodian\",\n",
    "    \"F\": \"Filipino\",\n",
    "    \"G\": \"Guamanian\",\n",
    "    \"H\": \"Hispanic/Latin/Mexican\",\n",
    "    \"I\": \"American Indian/Alaskan Native\",\n",
    "    \"J\": \"Japanese\",\n",
    "    \"K\": \"Korean\",\n",
    "    \"L\": \"Laotian\",\n",
    "    \"O\": \"Other\",\n",
    "    \"P\": \"Pacific Islander\",\n",
    "    \"S\": \"Samoan\",\n",
    "    \"U\": \"Hawaiian\",\n",
    "    \"V\": \"Vietnamese\",\n",
    "    \"W\": \"White\",\n",
    "    \"X\": \"Unknown\",\n",
    "    \"Z\": \"Asian Indian\"\n",
    "}\n",
    "\n",
    "count_top3 = count_top3.withColumn(\"Victim Descent\", col(\"Vict Descent\").cast(\"string\")).replace(descent_mapping, subset=[\"Victim Descent\"]).drop(\"Vict Descent\")\n",
    "columns_order = [\"Victim Descent\", \"count\"] \n",
    "count_top3 = count_top3.select(columns_order)\n",
    "count_tail3 = count_tail3.withColumn(\"Victim Descent\", col(\"Vict Descent\").cast(\"string\")).replace(descent_mapping, subset=[\"Victim Descent\"]).drop(\"Vict Descent\")\n",
    "count_tail3 = count_tail3.select(columns_order)\n",
    "\n",
    "count_tail3.show()\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
