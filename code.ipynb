{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Spark \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/29 12:08:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/29 12:08:25 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('Project') \\\n",
    "        .config(\"spark.master\", \"yarn\") \\\n",
    "        .config(\"spark.executor.instances\", \"4\") \\\n",
    "        .config(\"spark.executor.cores\", \"1\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŒñŒ∑œÑŒøœçŒºŒµŒΩŒø 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df1.union(df2)\n",
    "\n",
    "df = df.withColumn(\"Date Rptd\", to_date(col(\"Date Rptd\"), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df = df.withColumn(\"DATE OCC\", to_date(col(\"DATE OCC\"), 'MM/dd/yyyy hh:mm:ss a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2988445"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of rows in the DataFrame:\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŒñŒ∑œÑŒøœçŒºŒµŒΩŒø 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 199:============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+----------+\n",
      "|Year|Month|crime_total|row_number|\n",
      "+----+-----+-----------+----------+\n",
      "|2010|3    |17595      |1         |\n",
      "|2010|7    |17520      |2         |\n",
      "|2010|5    |17338      |3         |\n",
      "|2011|8    |17139      |1         |\n",
      "|2011|5    |17050      |2         |\n",
      "|2011|3    |16951      |3         |\n",
      "|2012|8    |17696      |1         |\n",
      "|2012|10   |17477      |2         |\n",
      "|2012|5    |17391      |3         |\n",
      "|2013|8    |17329      |1         |\n",
      "|2013|7    |16714      |2         |\n",
      "|2013|5    |16671      |3         |\n",
      "|2014|10   |12789      |1         |\n",
      "|2014|7    |12696      |2         |\n",
      "|2014|9    |12498      |3         |\n",
      "|2015|8    |18951      |1         |\n",
      "|2015|10   |18916      |2         |\n",
      "|2015|7    |18528      |3         |\n",
      "|2016|8    |19779      |1         |\n",
      "|2016|10   |19615      |2         |\n",
      "+----+-----+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, count, row_number\n",
    "from pyspark.sql import Window\n",
    "\n",
    "date_rptd = df.select('Date Rptd')\n",
    "date_rptd = date_rptd.withColumn(\"Year\", year(\"Date Rptd\")).withColumn(\"Month\", month(\"Date Rptd\")).drop(\"Date Rptd\")\n",
    "\n",
    "\n",
    "crime_total = date_rptd.groupBy(\"Year\", \"Month\").agg(count(\"*\").alias(\"crime_total\"))\n",
    "\n",
    "# Define a window specification to partition by the \"Year\" column and order by the \"crime_total\" column\n",
    "window_spec = Window().partitionBy(\"Year\").orderBy(col(\"crime_total\").desc())\n",
    "\n",
    "# Use the row_number function to assign row numbers within each group\n",
    "df_sorted = crime_total.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to keep only the top three within each group\n",
    "df_top_three_DF = df_sorted.filter(col(\"row_number\") <= 3)\n",
    "\n",
    "df_top_three_DF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['hadoop', 'fs', '-copyToLocal', 'hdfs://okeanos-master:54310/user/user/results/q1Dt.csv', '/home/user/Project/results/'], returncode=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_top_three_DF \\\n",
    "  .coalesce(1) \\\n",
    "  .write \\\n",
    "  .mode('overwrite') \\\n",
    "  .option('header', 'true') \\\n",
    "  .csv('results/q1Dt.csv')\n",
    "# df_top_three_DF.write.csv(\"results/q1Dt.csv\", header=True,  mode=\"overwrite\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "hdfs_path = \"hdfs://okeanos-master:54310/user/user/results/q1Dt.csv\"\n",
    "local_path = \"/home/user/Project/results/\"\n",
    "\n",
    "subprocess.run([\"hadoop\", \"fs\", \"-copyToLocal\", hdfs_path, local_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:================================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+----------+\n",
      "|Year|Month|crime_total|row_number|\n",
      "+----+-----+-----------+----------+\n",
      "|2010|3    |17595      |1         |\n",
      "|2010|7    |17520      |2         |\n",
      "|2010|5    |17338      |3         |\n",
      "|2011|8    |17139      |1         |\n",
      "|2011|5    |17050      |2         |\n",
      "|2011|3    |16951      |3         |\n",
      "|2012|8    |17696      |1         |\n",
      "|2012|10   |17477      |2         |\n",
      "|2012|5    |17391      |3         |\n",
      "|2013|8    |17329      |1         |\n",
      "|2013|7    |16714      |2         |\n",
      "|2013|5    |16671      |3         |\n",
      "|2014|10   |12789      |1         |\n",
      "|2014|7    |12696      |2         |\n",
      "|2014|9    |12498      |3         |\n",
      "|2015|8    |18951      |1         |\n",
      "|2015|10   |18916      |2         |\n",
      "|2015|7    |18528      |3         |\n",
      "|2016|8    |19779      |1         |\n",
      "|2016|10   |19615      |2         |\n",
      "+----+-----+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary SQL table\n",
    "df.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# Write the SQL query\n",
    "sql_query = \"\"\"\n",
    "    SELECT Year, Month, crime_total, row_number\n",
    "    FROM (\n",
    "        SELECT Year, Month, crime_total,\n",
    "               ROW_NUMBER() OVER (PARTITION BY Year ORDER BY crime_total DESC) AS row_number\n",
    "        FROM (\n",
    "            SELECT YEAR(`Date Rptd`) AS Year, MONTH(`Date Rptd`) AS Month, COUNT(*) AS crime_total\n",
    "            FROM crime_data\n",
    "            GROUP BY Year, Month\n",
    "        ) tmp\n",
    "    ) tmp2\n",
    "    WHERE row_number <= 3\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "df_top_three_sql = spark.sql(sql_query)\n",
    "\n",
    "# Show the result\n",
    "df_top_three_sql.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['hadoop', 'fs', '-copyToLocal', 'hdfs://okeanos-master:54310/user/user/results/q1SQL.csv', '/home/user/Project/results/'], returncode=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_top_three_sql.write.csv(\"results/q1SQL.csv\", header=True,  mode=\"overwrite\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "hdfs_path = \"hdfs://okeanos-master:54310/user/user/results/q1SQL.csv\"\n",
    "local_path = \"/home/user/Project/results/\"\n",
    "\n",
    "subprocess.run([\"hadoop\", \"fs\", \"-copyToLocal\", hdfs_path, local_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 129:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrames are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "is_same = df_top_three_DF.exceptAll(df_top_three_sql).count() == 0\n",
    "if is_same:\n",
    "    print(\"The DataFrames are identical.\")\n",
    "else:\n",
    "    print(\"The DataFrames are different.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŒñŒ∑œÑŒøœçŒºŒµŒΩŒø 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|TIME OCC|Premis Cd|\n",
      "+--------+---------+\n",
      "|13:50:00|      501|\n",
      "|00:45:00|      101|\n",
      "|15:15:00|      103|\n",
      "|01:50:00|      101|\n",
      "|21:00:00|      103|\n",
      "|16:50:00|      404|\n",
      "|20:05:00|      101|\n",
      "|21:00:00|      710|\n",
      "|02:30:00|      108|\n",
      "|21:00:00|      710|\n",
      "|14:45:00|      101|\n",
      "|20:00:00|      101|\n",
      "|02:45:00|      102|\n",
      "|17:45:00|      738|\n",
      "|20:30:00|      102|\n",
      "|17:35:00|      103|\n",
      "|12:25:00|      502|\n",
      "|11:00:00|      101|\n",
      "|20:00:00|      502|\n",
      "|18:20:00|      102|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, from_unixtime, date_format\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=False).select(\"TIME OCC\",\"Premis Cd\")\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=False).select(\"TIME OCC\",\"Premis Cd\")\n",
    "\n",
    "df = df1.union(df2)\n",
    "\n",
    "\n",
    "# Convert the 'TIME OCC' column to a timestamp\n",
    "df = df.withColumn(\n",
    "    \"TIME OCC\",\n",
    "    from_unixtime(unix_timestamp(col(\"TIME OCC\"), \"HHmm\")).cast(\"timestamp\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"TIME OCC\",\n",
    "    date_format(col(\"TIME OCC\").cast(\"timestamp\"), \"HH:mm:ss\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Premis Cd\",\n",
    "    col(\"Premis Cd\").cast(\"int\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================================>             (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+------+\n",
      "|Morning|Afternoon|Evening|Night |\n",
      "+-------+---------+-------+------+\n",
      "|123748 |148077   |186896 |237137|\n",
      "+-------+---------+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, sum\n",
    "\n",
    "filtered_df = df.filter(col(\"Premis Cd\") == 101).select(\"TIME OCC\")\n",
    "\n",
    "# Define time intervals\n",
    "morning_interval = ((col(\"TIME OCC\") >= \"05:00:00\") & (col(\"TIME OCC\") < \"12:00:00\"))\n",
    "afternoon_interval = ((col(\"TIME OCC\") >= \"12:00:00\") & (col(\"TIME OCC\") < \"17:00:00\"))\n",
    "evening_interval = ((col(\"TIME OCC\") >= \"17:00:00\") & (col(\"TIME OCC\") < \"21:00:00\"))\n",
    "night_interval = ((col(\"TIME OCC\") >= \"21:00:00\") | (col(\"TIME OCC\") < \"05:00:00\"))\n",
    "\n",
    "# Apply conditions and sum within each interval\n",
    "result_df = filtered_df.groupBy().agg(\n",
    "    sum(when(morning_interval, 1).otherwise(0)).alias(\"Morning\"),\n",
    "    sum(when(afternoon_interval, 1).otherwise(0)).alias(\"Afternoon\"),\n",
    "    sum(when(evening_interval, 1).otherwise(0)).alias(\"Evening\"),\n",
    "    sum(when(night_interval, 1).otherwise(0)).alias(\"Night\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[('Afternoon', 126476), ('Night', 205687), ('Morning', 107927), ('Evening', 165672)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/27 23:23:05 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Afternoon', 148077), ('Night', 237137), ('Morning', 123748), ('Evening', 186896)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import csv\n",
    "\n",
    "# Load the first CSV file into an RDD\n",
    "rdd1 = spark.textFile(\"hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv\") \\\n",
    "    .map(lambda x: next(csv.reader([x])))\n",
    "    \n",
    "header1 = rdd1.first()\n",
    "rdd1 = rdd1.filter(lambda row: row != header1)\n",
    "\n",
    "# Load the second CSV file into an RDD\n",
    "rdd2 = spark.textFile(\"hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv\") \\\n",
    "    .map(lambda x: next(csv.reader([x])))\n",
    "\n",
    "    \n",
    "header2 = rdd2.first()  \n",
    "rdd2 = rdd2.filter(lambda row: row != header2)\n",
    "\n",
    "# Merge the two RDDs\n",
    "rdd = rdd1.union(rdd2)\n",
    "rdd = rdd.map(lambda col: (col[3], col[14]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filtered_rdd = rdd.filter(lambda row: (row[1] == '101') or (row[1] == 101) )\n",
    "\n",
    "def get_interval(time_occ):\n",
    "    # Convert the time_occ to a datetime object for easier comparison\n",
    "    time_object = datetime.datetime.strptime(time_occ, \"%H%M\")\n",
    "\n",
    "    if datetime.time(5, 0) <= time_object.time() < datetime.time(12, 0):\n",
    "        return \"Morning\"\n",
    "    elif datetime.time(12, 0) <= time_object.time() < datetime.time(17, 0):\n",
    "        return \"Afternoon\"\n",
    "    elif datetime.time(17, 0) <= time_object.time() < datetime.time(21, 0):\n",
    "        return \"Evening\"\n",
    "    elif (datetime.time(21, 0) <= time_object.time()) or  (time_object.time() < datetime.time(5, 0)):\n",
    "        return \"Night\"\n",
    "\n",
    "    \n",
    "# Map each row to a tuple of (interval, 1)\n",
    "mapped_rdd = filtered_rdd.map(lambda col: (get_interval(col[0]), 1))\n",
    "\n",
    "# Reduce by key to sum occurrences within each interval\n",
    "result_rdd = mapped_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "print(result_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŒñŒ∑œÑŒøœçŒºŒµŒΩŒø 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, regexp_replace\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df1.union(df2)\n",
    "df = df.select('Date Rptd', 'Vict Descent','LAT', 'LON').filter(col('Vict Descent').isNotNull())\n",
    "df = df.withColumn(\"Date Rptd\", to_date(col(\"Date Rptd\"), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df = df.withColumn(\"Year\", year(\"Date Rptd\")).drop(\"Date Rptd\").filter(col(\"Year\") == 2015).drop('Year')\n",
    "\n",
    "income = spark.read.csv('hdfs://okeanos-master:54310/user/project/income/LA_income_2015.csv', header=True, inferSchema=True)\n",
    "income = income.withColumn(\"Estimated Median Income\", regexp_replace(\"Estimated Median Income\", \"[^0-9]\", \"\").cast(\"int\"))\n",
    "geocoding = spark.read.csv('hdfs://okeanos-master:54310/user/project/revgecoding.csv', header=True, inferSchema=True)\n",
    "\n",
    "geocoding = geocoding \\\n",
    "    .withColumnRenamed(\"LAT\", \"LAT_g\") \\\n",
    "    .withColumnRenamed(\"LON\", \"LON_g\") \n",
    "\n",
    "df = df.join(geocoding, (df.LAT == geocoding.LAT_g) & (df.LON == geocoding.LON_g)).drop(\"LAT\",\"LON\",\"LAT_g\",\"LON_g\")\n",
    "\n",
    "# non_matching_rows = df.join(geocoding, (df.LAT == geocoding.LAT_g) & (df.LON == geocoding.LON_g), \"left_anti\")\n",
    "\n",
    "distinct_geocoding = geocoding.select(\"ZIPcode\").distinct()\n",
    "filtered_income = income.join(distinct_geocoding, income[\"Zip Code\"] == distinct_geocoding[\"ZIPcode\"])\n",
    "filtered_income = filtered_income.orderBy(col(\"Estimated Median Income\").desc())\n",
    "\n",
    "# Get top 3 zip codes with highest Estimated Median Income\n",
    "top3 = filtered_income.limit(3)\n",
    "\n",
    "# Get bottom 3 zip codes with lowest Estimated Median Income\n",
    "filtered_income = filtered_income.orderBy(col(\"Estimated Median Income\"))\n",
    "tail3 = filtered_income.limit(3)\n",
    "\n",
    "join_top3 = df.join(top3, (df.ZIPcode == top3.ZIPcode))\n",
    "count_top3 = join_top3.groupBy(\"Vict Descent\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "join_tail3 = df.join(tail3, (df.ZIPcode == top3.ZIPcode))\n",
    "count_tail3 = join_tail3.groupBy(\"Vict Descent\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "descent_mapping = {\n",
    "    \"A\": \"Other Asian\",\n",
    "    \"B\": \"Black\",\n",
    "    \"C\": \"Chinese\",\n",
    "    \"D\": \"Cambodian\",\n",
    "    \"F\": \"Filipino\",\n",
    "    \"G\": \"Guamanian\",\n",
    "    \"H\": \"Hispanic/Latin/Mexican\",\n",
    "    \"I\": \"American Indian/Alaskan Native\",\n",
    "    \"J\": \"Japanese\",\n",
    "    \"K\": \"Korean\",\n",
    "    \"L\": \"Laotian\",\n",
    "    \"O\": \"Other\",\n",
    "    \"P\": \"Pacific Islander\",\n",
    "    \"S\": \"Samoan\",\n",
    "    \"U\": \"Hawaiian\",\n",
    "    \"V\": \"Vietnamese\",\n",
    "    \"W\": \"White\",\n",
    "    \"X\": \"Unknown\",\n",
    "    \"Z\": \"Asian Indian\"\n",
    "}\n",
    "\n",
    "count_top3 = count_top3.withColumn(\"Victim Descent\", col(\"Vict Descent\").cast(\"string\")).replace(descent_mapping, subset=[\"Victim Descent\"]).drop(\"Vict Descent\")\n",
    "columns_order = [\"Victim Descent\", \"count\"] \n",
    "count_top3 = count_top3.select(columns_order)\n",
    "count_tail3 = count_tail3.withColumn(\"Victim Descent\", col(\"Vict Descent\").cast(\"string\")).replace(descent_mapping, subset=[\"Victim Descent\"]).drop(\"Vict Descent\")\n",
    "count_tail3 = count_tail3.select(columns_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|Victim Descent        |count|\n",
      "+----------------------+-----+\n",
      "|White                 |312  |\n",
      "|Other                 |102  |\n",
      "|Hispanic/Latin/Mexican|53   |\n",
      "|Unknown               |26   |\n",
      "|Other Asian           |16   |\n",
      "|Black                 |14   |\n",
      "+----------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_top3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|Victim Descent                |count|\n",
      "+------------------------------+-----+\n",
      "|Hispanic/Latin/Mexican        |1503 |\n",
      "|Black                         |1078 |\n",
      "|White                         |690  |\n",
      "|Other                         |382  |\n",
      "|Other Asian                   |100  |\n",
      "|Unknown                       |63   |\n",
      "|Korean                        |7    |\n",
      "|American Indian/Alaskan Native|3    |\n",
      "|Japanese                      |3    |\n",
      "|Chinese                       |2    |\n",
      "|Filipino                      |1    |\n",
      "+------------------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_tail3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 380:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|Vict Descent|count|\n",
      "+------------+-----+\n",
      "|           H| 1503|\n",
      "|           B| 1078|\n",
      "|           W|  690|\n",
      "|           O|  382|\n",
      "|           A|  100|\n",
      "|           X|   63|\n",
      "|           K|    7|\n",
      "|           I|    3|\n",
      "|           J|    3|\n",
      "|           C|    2|\n",
      "|           F|    1|\n",
      "+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_tail3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŒñŒ∑œÑŒøœçŒºŒµŒΩŒø 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/29 15:35:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/29 15:35:19 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = \"./environment/bin/python\"\n",
    "spark = SparkSession.builder.config(\n",
    "    \"spark.archives\",  # 'spark.yarn.dist.archives' in YARN.\n",
    "    \"pyspark_conda_env.tar.gz#environment\")\\\n",
    "        .config(\"spark.master\", \"yarn\") \\\n",
    "        .config(\"spark.executor.instances\", \"2\") \\\n",
    "        .config(\"spark.executor.cores\", \"1\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, year,udf, mean,format_number, count\n",
    "from pyspark.sql.types import  FloatType\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df1.union(df2)\n",
    "df = df.withColumn(\"Date Rptd\", to_date(col(\"Date Rptd\"), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df = df.withColumn(\"Year\", year(\"Date Rptd\")).drop(\"Date Rptd\")\n",
    "\n",
    "df = df.select(\"Year\",\"AREA \",\"Weapon Used Cd\", \"LAT\", \"LON\")\n",
    "# filter fire arm crimes\n",
    "df = df.filter(df[\"Weapon Used Cd\"].cast(\"int\").between(100, 199))\n",
    "\n",
    "# remove Null Island entries\n",
    "df = df.filter((df[\"LAT\"] != 0) & (df[\"LON\"] != 0))\n",
    "\n",
    "police_stations = spark.read.csv('hdfs://okeanos-master:54310/user/project/LAPD_Police_Stations.csv', header=True, inferSchema=True)\n",
    "\n",
    "police_stations = police_stations.select(\"DIVISION\",\"X\", \"Y\",\"PREC\")\n",
    "\n",
    "df = df.join(police_stations, (df[\"AREA \"] == police_stations.PREC)).drop(\"AREA \", \"Weapon Used Cd\",\"PREC\")\n",
    "\n",
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    distance_km = geodesic((lat1, lon1), (lat2, lon2)).km\n",
    "    rounded_distance_km = round(distance_km, 3)\n",
    "    return rounded_distance_km\n",
    "\n",
    "get_distance = udf(get_distance, FloatType())\n",
    "\n",
    "distance_df = df.withColumn(\"Distance\", get_distance(col(\"LAT\"), col(\"LON\"), col(\"Y\"), col(\"X\")))\n",
    "\n",
    "result_a = distance_df.groupBy(\"Year\").agg(\n",
    "    mean(\"Distance\").alias(\"Mean_Distance (km)\"),\n",
    "    count(\"*\").alias(\"#\")\n",
    ").orderBy(col(\"Year\"))\n",
    "\n",
    "result_a = result_a.withColumn(\"Mean_Distance (km)\", format_number(\"Mean_Distance (km)\", 3))\n",
    "\n",
    "result_b = distance_df.groupBy(\"DIVISION\").agg(\n",
    "    mean(\"Distance\").alias(\"Mean_Distance (km)\"),\n",
    "    count(\"*\").alias(\"#\")\n",
    ").orderBy(col(\"#\").desc())\n",
    "\n",
    "result_b = result_b.withColumn(\"Mean_Distance (km)\", format_number(\"Mean_Distance (km)\", 3))\n",
    "\n",
    "result_a.show()\n",
    "\n",
    "result_b.show(result_b.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ŒñŒ∑œÑŒøœçŒºŒµŒΩŒø 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
