{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ Spark \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/28 20:57:44 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName('Project') \\\n",
    "        .config(\"spark.master\", \"yarn\") \\\n",
    "        .config(\"spark.executor.instances\", \"4\") \\\n",
    "        .config(\"spark.executor.cores\", \"1\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Î–Î·Ï„Î¿ÏÎ¼ÎµÎ½Î¿ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df1.union(df2)\n",
    "\n",
    "df = df.withColumn(\"Date Rptd\", to_date(col(\"Date Rptd\"), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df = df.withColumn(\"DATE OCC\", to_date(col(\"DATE OCC\"), 'MM/dd/yyyy hh:mm:ss a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2988445"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of rows in the DataFrame:\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Î–Î·Ï„Î¿ÏÎ¼ÎµÎ½Î¿ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 199:============================================>            (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+----------+\n",
      "|Year|Month|crime_total|row_number|\n",
      "+----+-----+-----------+----------+\n",
      "|2010|3    |17595      |1         |\n",
      "|2010|7    |17520      |2         |\n",
      "|2010|5    |17338      |3         |\n",
      "|2011|8    |17139      |1         |\n",
      "|2011|5    |17050      |2         |\n",
      "|2011|3    |16951      |3         |\n",
      "|2012|8    |17696      |1         |\n",
      "|2012|10   |17477      |2         |\n",
      "|2012|5    |17391      |3         |\n",
      "|2013|8    |17329      |1         |\n",
      "|2013|7    |16714      |2         |\n",
      "|2013|5    |16671      |3         |\n",
      "|2014|10   |12789      |1         |\n",
      "|2014|7    |12696      |2         |\n",
      "|2014|9    |12498      |3         |\n",
      "|2015|8    |18951      |1         |\n",
      "|2015|10   |18916      |2         |\n",
      "|2015|7    |18528      |3         |\n",
      "|2016|8    |19779      |1         |\n",
      "|2016|10   |19615      |2         |\n",
      "+----+-----+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, count, row_number\n",
    "from pyspark.sql import Window\n",
    "\n",
    "date_rptd = df.select('Date Rptd')\n",
    "date_rptd = date_rptd.withColumn(\"Year\", year(\"Date Rptd\")).withColumn(\"Month\", month(\"Date Rptd\")).drop(\"Date Rptd\")\n",
    "\n",
    "\n",
    "crime_total = date_rptd.groupBy(\"Year\", \"Month\").agg(count(\"*\").alias(\"crime_total\"))\n",
    "\n",
    "# Define a window specification to partition by the \"Year\" column and order by the \"crime_total\" column\n",
    "window_spec = Window().partitionBy(\"Year\").orderBy(col(\"crime_total\").desc())\n",
    "\n",
    "# Use the row_number function to assign row numbers within each group\n",
    "df_sorted = crime_total.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "\n",
    "# Filter to keep only the top three within each group\n",
    "df_top_three_DF = df_sorted.filter(col(\"row_number\") <= 3)\n",
    "\n",
    "df_top_three_DF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['hadoop', 'fs', '-copyToLocal', 'hdfs://okeanos-master:54310/user/user/results/q1Dt.csv', '/home/user/Project/results/'], returncode=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_top_three_DF \\\n",
    "  .coalesce(1) \\\n",
    "  .write \\\n",
    "  .mode('overwrite') \\\n",
    "  .option('header', 'true') \\\n",
    "  .csv('results/q1Dt.csv')\n",
    "# df_top_three_DF.write.csv(\"results/q1Dt.csv\", header=True,  mode=\"overwrite\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "hdfs_path = \"hdfs://okeanos-master:54310/user/user/results/q1Dt.csv\"\n",
    "local_path = \"/home/user/Project/results/\"\n",
    "\n",
    "subprocess.run([\"hadoop\", \"fs\", \"-copyToLocal\", hdfs_path, local_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:================================>                         (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----------+----------+\n",
      "|Year|Month|crime_total|row_number|\n",
      "+----+-----+-----------+----------+\n",
      "|2010|3    |17595      |1         |\n",
      "|2010|7    |17520      |2         |\n",
      "|2010|5    |17338      |3         |\n",
      "|2011|8    |17139      |1         |\n",
      "|2011|5    |17050      |2         |\n",
      "|2011|3    |16951      |3         |\n",
      "|2012|8    |17696      |1         |\n",
      "|2012|10   |17477      |2         |\n",
      "|2012|5    |17391      |3         |\n",
      "|2013|8    |17329      |1         |\n",
      "|2013|7    |16714      |2         |\n",
      "|2013|5    |16671      |3         |\n",
      "|2014|10   |12789      |1         |\n",
      "|2014|7    |12696      |2         |\n",
      "|2014|9    |12498      |3         |\n",
      "|2015|8    |18951      |1         |\n",
      "|2015|10   |18916      |2         |\n",
      "|2015|7    |18528      |3         |\n",
      "|2016|8    |19779      |1         |\n",
      "|2016|10   |19615      |2         |\n",
      "+----+-----+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Register the DataFrame as a temporary SQL table\n",
    "df.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# Write the SQL query\n",
    "sql_query = \"\"\"\n",
    "    SELECT Year, Month, crime_total, row_number\n",
    "    FROM (\n",
    "        SELECT Year, Month, crime_total,\n",
    "               ROW_NUMBER() OVER (PARTITION BY Year ORDER BY crime_total DESC) AS row_number\n",
    "        FROM (\n",
    "            SELECT YEAR(`Date Rptd`) AS Year, MONTH(`Date Rptd`) AS Month, COUNT(*) AS crime_total\n",
    "            FROM crime_data\n",
    "            GROUP BY Year, Month\n",
    "        ) tmp\n",
    "    ) tmp2\n",
    "    WHERE row_number <= 3\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "df_top_three_sql = spark.sql(sql_query)\n",
    "\n",
    "# Show the result\n",
    "df_top_three_sql.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['hadoop', 'fs', '-copyToLocal', 'hdfs://okeanos-master:54310/user/user/results/q1SQL.csv', '/home/user/Project/results/'], returncode=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_top_three_sql.write.csv(\"results/q1SQL.csv\", header=True,  mode=\"overwrite\")\n",
    "\n",
    "import subprocess\n",
    "\n",
    "hdfs_path = \"hdfs://okeanos-master:54310/user/user/results/q1SQL.csv\"\n",
    "local_path = \"/home/user/Project/results/\"\n",
    "\n",
    "subprocess.run([\"hadoop\", \"fs\", \"-copyToLocal\", hdfs_path, local_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 129:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrames are identical.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "is_same = df_top_three_DF.exceptAll(df_top_three_sql).count() == 0\n",
    "if is_same:\n",
    "    print(\"The DataFrames are identical.\")\n",
    "else:\n",
    "    print(\"The DataFrames are different.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Î–Î·Ï„Î¿ÏÎ¼ÎµÎ½Î¿ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|TIME OCC|Premis Cd|\n",
      "+--------+---------+\n",
      "|13:50:00|      501|\n",
      "|00:45:00|      101|\n",
      "|15:15:00|      103|\n",
      "|01:50:00|      101|\n",
      "|21:00:00|      103|\n",
      "|16:50:00|      404|\n",
      "|20:05:00|      101|\n",
      "|21:00:00|      710|\n",
      "|02:30:00|      108|\n",
      "|21:00:00|      710|\n",
      "|14:45:00|      101|\n",
      "|20:00:00|      101|\n",
      "|02:45:00|      102|\n",
      "|17:45:00|      738|\n",
      "|20:30:00|      102|\n",
      "|17:35:00|      103|\n",
      "|12:25:00|      502|\n",
      "|11:00:00|      101|\n",
      "|20:00:00|      502|\n",
      "|18:20:00|      102|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp, from_unixtime, date_format\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=False).select(\"TIME OCC\",\"Premis Cd\")\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=False).select(\"TIME OCC\",\"Premis Cd\")\n",
    "\n",
    "df = df1.union(df2)\n",
    "\n",
    "\n",
    "# Convert the 'TIME OCC' column to a timestamp\n",
    "df = df.withColumn(\n",
    "    \"TIME OCC\",\n",
    "    from_unixtime(unix_timestamp(col(\"TIME OCC\"), \"HHmm\")).cast(\"timestamp\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"TIME OCC\",\n",
    "    date_format(col(\"TIME OCC\").cast(\"timestamp\"), \"HH:mm:ss\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Premis Cd\",\n",
    "    col(\"Premis Cd\").cast(\"int\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=============================================>             (7 + 2) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+------+\n",
      "|Morning|Afternoon|Evening|Night |\n",
      "+-------+---------+-------+------+\n",
      "|123748 |148077   |186896 |237137|\n",
      "+-------+---------+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, sum\n",
    "\n",
    "filtered_df = df.filter(col(\"Premis Cd\") == 101).select(\"TIME OCC\")\n",
    "\n",
    "# Define time intervals\n",
    "morning_interval = ((col(\"TIME OCC\") >= \"05:00:00\") & (col(\"TIME OCC\") < \"12:00:00\"))\n",
    "afternoon_interval = ((col(\"TIME OCC\") >= \"12:00:00\") & (col(\"TIME OCC\") < \"17:00:00\"))\n",
    "evening_interval = ((col(\"TIME OCC\") >= \"17:00:00\") & (col(\"TIME OCC\") < \"21:00:00\"))\n",
    "night_interval = ((col(\"TIME OCC\") >= \"21:00:00\") | (col(\"TIME OCC\") < \"05:00:00\"))\n",
    "\n",
    "# Apply conditions and sum within each interval\n",
    "result_df = filtered_df.groupBy().agg(\n",
    "    sum(when(morning_interval, 1).otherwise(0)).alias(\"Morning\"),\n",
    "    sum(when(afternoon_interval, 1).otherwise(0)).alias(\"Afternoon\"),\n",
    "    sum(when(evening_interval, 1).otherwise(0)).alias(\"Evening\"),\n",
    "    sum(when(night_interval, 1).otherwise(0)).alias(\"Night\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[('Afternoon', 126476), ('Night', 205687), ('Morning', 107927), ('Evening', 165672)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/27 23:23:05 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"RDD query\") \\\n",
    "    .getOrCreate() \\\n",
    "    .sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Afternoon', 148077), ('Night', 237137), ('Morning', 123748), ('Evening', 186896)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import csv\n",
    "\n",
    "# Load the first CSV file into an RDD\n",
    "rdd1 = spark.textFile(\"hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv\") \\\n",
    "    .map(lambda x: next(csv.reader([x])))\n",
    "    \n",
    "header1 = rdd1.first()\n",
    "rdd1 = rdd1.filter(lambda row: row != header1)\n",
    "\n",
    "# Load the second CSV file into an RDD\n",
    "rdd2 = spark.textFile(\"hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv\") \\\n",
    "    .map(lambda x: next(csv.reader([x])))\n",
    "\n",
    "    \n",
    "header2 = rdd2.first()  \n",
    "rdd2 = rdd2.filter(lambda row: row != header2)\n",
    "\n",
    "# Merge the two RDDs\n",
    "rdd = rdd1.union(rdd2)\n",
    "rdd = rdd.map(lambda col: (col[3], col[14]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filtered_rdd = rdd.filter(lambda row: (row[1] == '101') or (row[1] == 101) )\n",
    "\n",
    "def get_interval(time_occ):\n",
    "    # Convert the time_occ to a datetime object for easier comparison\n",
    "    time_object = datetime.datetime.strptime(time_occ, \"%H%M\")\n",
    "\n",
    "    if datetime.time(5, 0) <= time_object.time() < datetime.time(12, 0):\n",
    "        return \"Morning\"\n",
    "    elif datetime.time(12, 0) <= time_object.time() < datetime.time(17, 0):\n",
    "        return \"Afternoon\"\n",
    "    elif datetime.time(17, 0) <= time_object.time() < datetime.time(21, 0):\n",
    "        return \"Evening\"\n",
    "    elif (datetime.time(21, 0) <= time_object.time()) or  (time_object.time() < datetime.time(5, 0)):\n",
    "        return \"Night\"\n",
    "\n",
    "    \n",
    "# Map each row to a tuple of (interval, 1)\n",
    "mapped_rdd = filtered_rdd.map(lambda col: (get_interval(col[0]), 1))\n",
    "\n",
    "# Reduce by key to sum occurrences within each interval\n",
    "result_rdd = mapped_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "print(result_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Î–Î·Ï„Î¿ÏÎ¼ÎµÎ½Î¿ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, regexp_replace\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df1.union(df2)\n",
    "df = df.select('Date Rptd', 'Vict Descent','LAT', 'LON').filter(col('Vict Descent').isNotNull())\n",
    "df = df.withColumn(\"Date Rptd\", to_date(col(\"Date Rptd\"), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df = df.withColumn(\"Year\", year(\"Date Rptd\")).drop(\"Date Rptd\").filter(col(\"Year\") == 2015).drop('Year')\n",
    "\n",
    "income = spark.read.csv('hdfs://okeanos-master:54310/user/project/income/LA_income_2015.csv', header=True, inferSchema=True)\n",
    "income = income.withColumn(\"Estimated Median Income\", regexp_replace(\"Estimated Median Income\", \"[^0-9]\", \"\").cast(\"int\"))\n",
    "geocoding = spark.read.csv('hdfs://okeanos-master:54310/user/project/revgecoding.csv', header=True, inferSchema=True)\n",
    "\n",
    "geocoding = geocoding \\\n",
    "    .withColumnRenamed(\"LAT\", \"LAT_g\") \\\n",
    "    .withColumnRenamed(\"LON\", \"LON_g\") \n",
    "\n",
    "df = df.join(geocoding, (df.LAT == geocoding.LAT_g) & (df.LON == geocoding.LON_g)).drop(\"LAT\",\"LON\",\"LAT_g\",\"LON_g\")\n",
    "\n",
    "# non_matching_rows = df.join(geocoding, (df.LAT == geocoding.LAT_g) & (df.LON == geocoding.LON_g), \"left_anti\")\n",
    "\n",
    "distinct_geocoding = geocoding.select(\"ZIPcode\").distinct()\n",
    "filtered_income = income.join(distinct_geocoding, income[\"Zip Code\"] == distinct_geocoding[\"ZIPcode\"])\n",
    "filtered_income = filtered_income.orderBy(col(\"Estimated Median Income\").desc())\n",
    "\n",
    "# Get top 3 zip codes with highest Estimated Median Income\n",
    "top3 = filtered_income.limit(3)\n",
    "\n",
    "# Get bottom 3 zip codes with lowest Estimated Median Income\n",
    "filtered_income = filtered_income.orderBy(col(\"Estimated Median Income\"))\n",
    "tail3 = filtered_income.limit(3)\n",
    "\n",
    "join_top3 = df.join(top3, (df.ZIPcode == top3.ZIPcode))\n",
    "count_top3 = join_top3.groupBy(\"Vict Descent\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "join_tail3 = df.join(tail3, (df.ZIPcode == top3.ZIPcode))\n",
    "count_tail3 = join_tail3.groupBy(\"Vict Descent\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "descent_mapping = {\n",
    "    \"A\": \"Other Asian\",\n",
    "    \"B\": \"Black\",\n",
    "    \"C\": \"Chinese\",\n",
    "    \"D\": \"Cambodian\",\n",
    "    \"F\": \"Filipino\",\n",
    "    \"G\": \"Guamanian\",\n",
    "    \"H\": \"Hispanic/Latin/Mexican\",\n",
    "    \"I\": \"American Indian/Alaskan Native\",\n",
    "    \"J\": \"Japanese\",\n",
    "    \"K\": \"Korean\",\n",
    "    \"L\": \"Laotian\",\n",
    "    \"O\": \"Other\",\n",
    "    \"P\": \"Pacific Islander\",\n",
    "    \"S\": \"Samoan\",\n",
    "    \"U\": \"Hawaiian\",\n",
    "    \"V\": \"Vietnamese\",\n",
    "    \"W\": \"White\",\n",
    "    \"X\": \"Unknown\",\n",
    "    \"Z\": \"Asian Indian\"\n",
    "}\n",
    "\n",
    "count_top3 = count_top3.withColumn(\"Victim Descent\", col(\"Vict Descent\").cast(\"string\")).replace(descent_mapping, subset=[\"Victim Descent\"]).drop(\"Vict Descent\")\n",
    "columns_order = [\"Victim Descent\", \"count\"] \n",
    "count_top3 = count_top3.select(columns_order)\n",
    "count_tail3 = count_tail3.withColumn(\"Victim Descent\", col(\"Vict Descent\").cast(\"string\")).replace(descent_mapping, subset=[\"Victim Descent\"]).drop(\"Vict Descent\")\n",
    "count_tail3 = count_tail3.select(columns_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 85:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-----+\n",
      "|Victim Descent        |count|\n",
      "+----------------------+-----+\n",
      "|White                 |312  |\n",
      "|Other                 |102  |\n",
      "|Hispanic/Latin/Mexican|53   |\n",
      "|Unknown               |26   |\n",
      "|Other Asian           |16   |\n",
      "|Black                 |14   |\n",
      "+----------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_top3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:===================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+\n",
      "|Victim Descent                |count|\n",
      "+------------------------------+-----+\n",
      "|Hispanic/Latin/Mexican        |1503 |\n",
      "|Black                         |1078 |\n",
      "|White                         |690  |\n",
      "|Other                         |382  |\n",
      "|Other Asian                   |100  |\n",
      "|Unknown                       |63   |\n",
      "|Korean                        |7    |\n",
      "|American Indian/Alaskan Native|3    |\n",
      "|Japanese                      |3    |\n",
      "|Chinese                       |2    |\n",
      "|Filipino                      |1    |\n",
      "+------------------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_tail3.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 380:==================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|Vict Descent|count|\n",
      "+------------+-----+\n",
      "|           H| 1503|\n",
      "|           B| 1078|\n",
      "|           W|  690|\n",
      "|           O|  382|\n",
      "|           A|  100|\n",
      "|           X|   63|\n",
      "|           K|    7|\n",
      "|           I|    3|\n",
      "|           J|    3|\n",
      "|           C|    2|\n",
      "|           F|    1|\n",
      "+------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count_tail3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Î–Î·Ï„Î¿ÏÎ¼ÎµÎ½Î¿ 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import geopy.distance\n",
    "\n",
    "from pyspark.sql.functions import col, to_date, year,udf\n",
    "from pyspark.sql.types import  FloatType\n",
    "\n",
    "def get_distance(lat1, lon1, lat2, lon2):\n",
    "    return geopy.distance.geodesic((lat1, lon1), (lat2, lon2)).km\n",
    "\n",
    "get_distance = udf(get_distance, FloatType())\n",
    "\n",
    "# Read the CSVs file into a DataFrames\n",
    "df1 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2010_to_2019.csv', header=True, inferSchema=True)\n",
    "df2 = spark.read.csv('hdfs://okeanos-master:54310/user/project/Crime_Data_from_2020_to_Present.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df1.union(df2)\n",
    "df = df.withColumn(\"Date Rptd\", to_date(col(\"Date Rptd\"), 'MM/dd/yyyy hh:mm:ss a'))\n",
    "df = df.withColumn(\"Year\", year(\"Date Rptd\")).drop(\"Date Rptd\")\n",
    "\n",
    "df = df.select(\"Year\",\"AREA \",\"Weapon Used Cd\", \"LAT\", \"LON\")\n",
    "# filter fire arm crimes\n",
    "df = df.filter(df[\"Weapon Used Cd\"].cast(\"int\").between(100, 199))\n",
    "\n",
    "# remove Null Island entries\n",
    "df = df.filter((df[\"LAT\"] != 0) & (df[\"LON\"] != 0))\n",
    "\n",
    "police_stations = spark.read.csv('hdfs://okeanos-master:54310/user/project/LAPD_Police_Stations.csv', header=True, inferSchema=True)\n",
    "\n",
    "police_stations = police_stations.select(\"X\", \"Y\",\"PREC\")\n",
    "\n",
    "join = df.join(police_stations, (df[\"AREA \"] == police_stations.PREC)).drop(\"AREA \", \"Weapon Used Cd\",\"PREC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_df = join.withColumn(\"Distance\", get_distance(col(\"LAT\"), col(\"LON\"), col(\"Y\"), col(\"X\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/28 21:20:59 WARN TaskSetManager: Lost task 0.0 in stage 79.0 (TID 176) (okeanos-worker executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 1231, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000002/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000002/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000002/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'geopy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/12/28 21:21:02 WARN TaskSetManager: Lost task 0.2 in stage 79.0 (TID 178) (okeanos-master executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000005/pyspark.zip/pyspark/worker.py\", line 1231, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000005/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000005/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000005/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000005/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000005/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000005/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'geopy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/12/28 21:21:03 WARN TaskSetManager: Lost task 0.3 in stage 79.0 (TID 179) (okeanos-worker executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/worker.py\", line 1231, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'geopy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/12/28 21:21:03 ERROR TaskSetManager: Task 0 in stage 79.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'geopy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdistance_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    954\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    955\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    956\u001b[0m     )\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/worker.py\", line 1231, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/worker.py\", line 1067, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/worker.py\", line 529, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/worker.py\", line 90, in read_command\n    command = serializer._read_with_length(file)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n  File \"/home/user/opt/data/hadoop/nm-local-dir/usercache/user/appcache/application_1703788084972_0005/container_1703788084972_0005_01_000004/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'geopy'\n"
     ]
    }
   ],
   "source": [
    "distance_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Î–Î·Ï„Î¿ÏÎ¼ÎµÎ½Î¿ 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
